# Gradient Boosting Classifier from First Principles

This project implements a Gradient Boosting Tree Classifier **entirely from first principles**, following the methodology described in **Sections 10.9â€“10.10 of *The Elements of Statistical Learning (2nd Edition)*.**

It includes a custom-built decision tree regressor as the weak learner and supports classification tasks via additive modeling in the functional space.

## ğŸ“ Project Structure

```
gradient_boosting/
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ decision_tree_regressor.py
â”‚   â””â”€â”€ gradient_boosting_classifier.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ classification.csv
â”‚   â””â”€â”€ regression.csv
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ tests_boosting.py
â”‚   â””â”€â”€ tests_boosting_CR.py
â”‚   â””â”€â”€ test_data_generator.py
â”‚   â””â”€â”€ test_multiclass.py
â”‚â”€â”€ generate_data.py
â”‚â”€â”€ GradientBoosting_Demo.ipynb
â”‚â”€â”€ README.md
```

---

## ğŸ” What does the model you have implemented do and when should it be used?

In addition to classification, our implementation also supports **regression tasks** using squared loss. When the `task='regression'` option is provided during initialization, the model will minimize mean squared error (MSE) at each stage of boosting. This flexibility allows our Gradient Boosting framework to be applied to both predictive modeling scenarios: classification and regression.



This model performs **classification** using **gradient boosting**, an ensemble learning technique that builds a strong classifier from an ensemble of weak learners (in this case, shallow decision trees). Automatically detects binary vs. multiclass and applies the appropriate (log-loss / soft-max) objective At each iteration, it fits a new tree to the **negative gradient** of the loss function with respect to the current prediction. This allows the model to focus more on misclassified or hard-to-predict samples.

It is suitable for classification tasks where:
- The relationship between features and target is complex and nonlinear.
- Interpretability or sparsity is less of a concern than **predictive performance**.
- You want to balance **bias and variance** through `learning_rate` and tree complexity.


## Data Generation

The CSV files used for training and testing (both for classification and regression) are automatically generated by the script `tests/test_data_generator.py`. 
This script uses internally defined functions to create synthetic datasets with controlled parameters and saves them to the `data/` directory. 

This allows consistent and reproducible testing of the models implemented, and ensures all students work from the same data conditions.

## Jupyter Notebook

We also include a Jupyter Notebook named `notebook_demo.ipynb`, which visually demonstrates the functionality and correctness of the Gradient Boosting algorithm. 
It shows how the algorithm learns over iterations, how it reduces error, and how decision boundaries evolve. This serves both as a testing and explanatory tool, making it easier to understand the modelâ€™s behavior.


---

## ğŸ§ª How did you test your model to determine if it is working reasonably correctly?

We wrote **custom unit tests** in `tests_boosting.py` and `tests_boosting_CR.py` and `test_multiclass.py`, which:
- Generate synthetic classification datasets.
- Check training accuracy and convergence.
- Compare performance for different learning rates and tree depths.
- Evaluate classification boundaries visually (optional in a notebook).

All tests run without requiring external machine learning libraries.

---

## âš™ï¸ What parameters have you exposed to users of your implementation in order to tune performance?

### Main Parameters:

| Parameter         | Description                                      |
|------------------|--------------------------------------------------|
| `n_estimators`    | Number of boosting rounds (trees)               |
| `learning_rate`   | Shrinkage applied to each tree's prediction     |
| `max_depth`       | Maximum depth of each decision tree             |
| `min_samples_split` | Minimum samples required to split a node      |
| `task`            | Either `'classification'` or `'regression'`     |

### Example Usage:

```python
from model.gradient_boosting_classifier import GradientBoostingClassifier
from generate_data import generate_classification_data

X_train, y_train = classification_data_generator(m, b, rnge, N, seed)

gb = GradientBoostingClassifier(n_estimators=10, learning_rate=0.1, max_depth=2)
gb.fit(X_train, y_train)
predictions = gb.predict(X_train)
```

---

## âš ï¸ Are there specific inputs that your implementation has trouble with?

- Very noisy datasets may require a **small learning rate** and more iterations to converge.
- Since trees are built recursively from scratch at each iteration, very large datasets can result in **longer training times**.

With more time, we could:
- Optimize tree-growing with pruning or histogram-based methods.
- Vectorize the gradient computation for better efficiency.

---

## ğŸš€ How to Run the Project

1. Clone the repository and navigate to the project root.
2. Install dependencies (only `numpy` and `pytest` are needed):
   ```
   pip install -r requirements.txt
   ```
3. Run tests:
   ```
   pytest tests/
   ```

---

## âœï¸ Authors

- **Arnau FitÃ© Cluet**
- **Susana FernÃ¡ndez Cavia**
- **Antonio Ardura Carnicero**

This implementation was developed as part of our coursework in Machine Learning Theory, Spring 2025.

---
